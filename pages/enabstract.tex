% 英文摘要

Multi-sensor fusion, which could compensate for the deficiencies of stand-alone sensors and provide better and more reliable services in complex environments, has been widely accepted as a feasible solution for navigation and perception tasks. For a multi-sensor system, correctly calibrated spatiotemporal parameters are highly desired to ensure fusion algorithms work properly, as inaccurate spatiotemporal parameters
would cause inconsistency among different sensors and impair
fusion performance. To this end, we propose a target-free LiDAR/Camera/IMU spatiotemporal calibration method based on continuous-time trajectory estimation, which estimates both spatial and temporal parameters using constraints constructed from environmental features without relying on targets or prior knowledge. To tightly incorporate the estimation of the temporal parameters alongside spatial parameters into a global factor graph, B-spline curves are employed to model the continuous-time trajectory. Compared to discrete-time methods, this continuous-time representation could achieve higher accuracy, especially in highly dynamic scenarios. To study the effect of different degenerate motions on the calibration accuracy, we performed a theoretical analysis and experimental verification for the system observability based on the Lie derivative and proved that the whole system is completely observable under random motion. Simulation and real-world experiments are carried out to verify the feasibility of the proposed method and evaluate its performance. The results demonstrate that our method could achieve high calibration accuracy of spatiotemporal parameters with the guarantee of system consistency.